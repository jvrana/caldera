{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Relative Imports\n",
    "##########################################################\n",
    "import sys\n",
    "from os.path import isfile\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "def find_pkg(name: str, depth: int):\n",
    "    if depth <= 0:\n",
    "        ret = None\n",
    "    else:\n",
    "        d = [\"..\"] * depth\n",
    "        path_parts = d + [name, \"__init__.py\"]\n",
    "\n",
    "        if isfile(join(*path_parts)):\n",
    "            ret = d\n",
    "        else:\n",
    "            ret = find_pkg(name, depth - 1)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def find_and_ins_syspath(name: str, depth: int):\n",
    "    path_parts = find_pkg(name, depth)\n",
    "    if path_parts is None:\n",
    "        raise RuntimeError(\"Could not find {}. Try increasing depth.\".format(name))\n",
    "    path = join(*path_parts)\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "\n",
    "try:\n",
    "    import caldera\n",
    "except ImportError:\n",
    "    find_and_ins_syspath(\"caldera\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caldera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caldera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data tour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphData\n",
    "\n",
    "The primary data object used by caldera. We can generate random objects by using `GraphData.random` for testing and demo purposes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GraphData size(n,e,g)=torch.Size([2, 8, 1]) features(n,e,g)=torch.Size([5, 4, 3])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from caldera.data import GraphData\n",
    "\n",
    "data = GraphData.random(n_feat=5, e_feat=4, g_feat=3)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GraphData` instances contain node tensor (`data.x`), edge tensor (`data.e`) and global tensor (`data.g`). They also keep graph topology\n",
    "via and edge list, `data.edges`, which indicates edges by node indices. Note that this mean graphs are MultiGraphs (multiple edges between same nodes allowed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### node features\n",
    "\n",
    "Each node gets a tensor. These tensors are stack into `data.x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 nodes\n",
      "Each node has a feature tensor of shape 5\n",
      "Overall shape: torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2496, -0.3646,  0.3904, -0.4723, -0.5130],\n",
       "        [-1.4683,  1.2624, -0.1099, -0.1572, -0.9023]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('We have {} nodes'.format(data.x.shape[0]))\n",
    "print('Each node has a feature tensor of shape {}'.format(data.x.shape[1]))\n",
    "print(\"Overall shape: {}\".format(data.x.shape))\n",
    "data.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### edge features\n",
    "\n",
    "Each edge gets a tensor. These tensors are stack into `data.e`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8 edge(s)\n",
      "Each edge has a feature tensor of shape 4\n",
      "Overall shape: torch.Size([8, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2496, -0.3646,  0.3904, -0.4723, -0.5130],\n",
       "        [-1.4683,  1.2624, -0.1099, -0.1572, -0.9023]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('We have {} edge(s)'.format(data.e.shape[0]))\n",
    "print('Each edge has a feature tensor of shape {}'.format(data.e.shape[1]))\n",
    "print(\"Overall shape: {}\".format(data.e.shape))\n",
    "data.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### global features\n",
    "\n",
    "Each graph gets a single global tensor. These tensors are stack into `data.g`. \n",
    "\n",
    "We may use this, for example, to label each graph with a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a single global tensor of shape torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1705, -1.5258, -0.8309]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('We have a single global tensor of shape {}'.format(data.g.shape))\n",
    "data.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### edges\n",
    "\n",
    "Graph topology is store in the `data.edges` attribute. This indicates which nodes (by index) are connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 1, 0, 1, 1, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion\n",
    "\n",
    "We can convert `GraphData` instances to and from networkx instances (graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GraphData size(n,e,g)=torch.Size([11, 3, 1]) features(n,e,g)=torch.Size([5, 4, 3])>\n",
      "tensor([[ 4,  7,  0],\n",
      "        [ 3,  6, 10]])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUj0lEQVR4nO3dUWhj2WGH8f+1pfF1YiuGHZMxa5OF1diahayp3cLAQix3SRv8EMjWA5swTyEkwUMa+tQSvxX8UELTvNiEXVIoOylZIkg3UNNOllruQzuwa29t2lo2KgyxEnsjmyqyGksjWbcPWu3OZHZkX90jy/L5fo8zvmfOLrP32yPde47jeZ4nAAAs0dXuCQAAcJ4IHwDAKoQPAGAVwgcAsArhAwBYhfABAKxC+AAAViF8AACrED4AgFUIHwDAKoQPAGAVwgcAsArhAwBYhfABAKxC+AAAViF8AACrED4AgFUIHwDAKoQPAGAVwgcAsArhAwBYJdTuCQCA7Q4KJSXWMkrt55UvVhRxQ4pdi+jW5LCe6etp9/QuHcfzPK/dkwAuO25s+DgbuzktJtNa3clKkkqV6oe/54a65EmKjw1qbiqq8ZGB9kzyEiJ8QAtxY8PT3L3/QAvLKRUrJ2p0F3YcyQ11a34mpts3nzu3+V1mhA9oEW5seJra340tHZerp//wB3rDXZqfucHfEQN4uAVogY9ubI2jJ0meJx2XT7SwvKW79x+cy/zQPhu7OS0sp3xFT5KOy1UtLKe0mcm1ZmIWIXyAYdzY0MhiMq1i5aSpa4uVEy0l04ZnZB/CBxjGjQ1Pc1AoaXUne+qnAE/jedLKdlaHhZLZiVmG8AEGcWNDI4m1TOAxHEmJ9eDj2IzwAQZxY0Mjqf38Y0/2NqNYqSq1d2RoRnYifIBB3NjQSL5YMTRO2cg4tiJ8gEHc2NBIxDWzWVbEDRsZx1aEDzCIGxsaiV2LqCcU7LbrhroUG+o3NCM7ET7AIG5saGR2cjjwGJ6k2Yng49iM8AEGcWNDI1f7ejQ1OijHae56x5GmxwbZ3zUgwgcYxI0Np7kTj8oNdTd1rRvq1lw8anhG9iF8gGHc2NDI+MiA5mdi6g37u/3W9uqM6cXhgdZMzCKEDzCMGxtOc/vmc5qfuaHecPepnw44jtQb7maDaoM4nQFoEU5nwGk2MzktJdNa2c7KUe0dzrr6sVXTY4Oai0f5HyKDCB/QQhu7/6s//cE/ar/rGW5seKrDQkmJ9YxSe0fKF8uKuGHFhvo1O8FBxa1A+IAW8TxPX/nKV/TjH/9Y//7ef2oj73JjAy4AM2/bAtBBoaTEWkap/bzyx2X913+8q/Sup57IM8rt7+obX/hCu6cIQKz4gMA2dnNaTKa1upOVpMf26qyWS3IcR9f7yvrrr/6RxkcG2jRLAHWEDwjgrA+wyKuq90qYB1iAC4DwAU2qRW/L10nrtVcWeCwdaCfe4wOasLGb08Jyylf0JOm4XNXCckqbmVxrJgbgVIQPaMJiMq1i5aSpa4uVEy0l04ZnBOCsCB/g00GhpNWdbOPv9BrwPGllO6vDQsnsxACcCeEDfEqsZQKP4UhKrAcfB4B/hA/wKbWff+yVhWYUK1Wl9o4MzQiAH4QP8ClfrBgap2xkHAD+ED7Ap4hrZsOjiBs2Mg4Afwgf4FPsWkQ9oWD/6bihLsWG+g3NCIAfhA/waXZyOPAYnqTZieDjAPCP8AE+Xe3r0dTo4KkHiD6N49SOIuJUBqA9CB/QhDvxqNxQd1PXuqFuzcWjhmcE4KwIH9CE8ZEBzc/E1Bv2959Qba/OGIfOAm3EeXxAk+obTZ/ldAbHqa30OJ0BaD9OZwAC2szktJRMa2U7K0e1l9Pr3FCXPNW+05uLR1npARcA4QMMOSyUlFjPKLV3pHyxrIgbVmyoX7MTwzzIAlwghA8AYBUebgEAWIXwAQCsQvgAAFYhfAAAqxA+AIBVCB8AwCqEDwBgFcIHALDKpdyr86BQUmIto9R+XvliRRE3pNi1iG5NsoMGANjuUu3csrGb02IyrdWdrCSp9DF7JsbHBjU3FdX4yEB7JgkAaKtLE7679x+wSz4A4FSX4qPOWvS2dFyunvqznicdl0+0sLwlScQPACzT8Q+3bOzmtLCcOlP0HnVcrmphOaXNTK41EwMAXEgdH77FZFrFyklT1xYrJ1pKpg3PCABwkXV0+A4KJa3uZBt+p9eI50kr21kdFkpmJwYAuLA6OnyJtUzgMRxJifXg4wAAOkNHhy+1n3/slYVmFCtVpfaODM0IAHDRdXT48sWKoXHKRsYBAFx8HR2+iGvmbYyIGzYyDgDg4uvo8MWuRdQTCvaP4Ia6FBvqNzQjAMBF19Hhm50cDjyGJ2l2Ivg4AIDO0NHhu9rXo6nRQTlOc9c7jjQ9NsjG1QBgkY4OnyTdiUflhrqbutYNdWsuHjU8IwDARdbx4RsfGdD8TEy9YX//KL3hLs3PxPTi8EBrJgYAuJAuxSbV9Y2mOZ0BAHCaS3MskSRtZnJaSqa1sp2Vo9rL6XX18/imxwY1F4+y0gMAS12q8NUdFkpKrGeU2jtSvlhWxA0rNtSv2QlOYAeAdjoolJRYyyi1n1e+WFHEDSl2LaJbk+d3f76U4QMAXCwbuzktJtNa3clK0mPbTdY/kYuPDWpuKqrxkYGWzoXwAQBaqnZY+MV5BuNSPNwCALiYatHbOtNh4Z4nHZdPtLC8JUkti1/Hv84AALiYNnZzWlhOnSl6jzouV7WwnNJmJteSeRE+AEBLLCbTKlZOmrq2WDnRUjJteEY1hA8AYNxBoaTVnWzD7/Qa8TxpZTurw0LJ7MRE+AAALZBYywQew5GUWA8+zu8ifAAA41L7+cdeWWhGsVJVau/I0Iw+QvgAAMblixVD45SNjPMowgcAMC7imnlbLuKGjYzzKMIHADAudi2inlCwxLihLsWG+g3N6COEDwBg3OzkcOAxPEmzE8HH+V2EDwBg3NW+Hk2NDspxmrvecWqn6bRi42rCBwBoiTvxqNxQd1PXuqFuzcWjhmdUQ/gAAC0xPjKg+ZmYesP+UtMb7tL8TKxl56aySTUAoGXqG01fpNMZOJYIANBym5mclpJprWxn5aj2cnpd/Ty+6bFBzcWjLVvp1RE+AMC5OSyUlFjPKLV3pHyxrIgbVmyoX7MTnMAOAEBL8HALAMAqhA8AYBXCBwCwCuEDAFiF8AEArEL4AABWIXwAAKsQPgCAVQgfAMAqhA8AYBXCBwCwCuEDAFiF8AEArEL4AABWIXwAAKsQPgCAVQgfAMAqhA8AYBXCBwCwCuEDAFiF8AEArEL4AABWIXwAAKsQPgCAVQgfAMAqhA8AYBXCBwCwCuEDAFiF8AEArEL4AABWIXwAAKsQPgCAVQgfAMAqhA8AYBXCBwCwCuEDAFiF8AEArEL4AABWIXwAAKsQPgCAVQgfAMAqhA8AYBXCBwCwCuEDAFiF8AEArEL4AABWIXwAAKsQPgCAVQgfAMAqhA8AYBXCBwCwCuEDAFiF8AEArEL4AABWIXwAAKsQPgCAVQgfAMAqhA8AYBXCBwCwCuEDAFiF8AEArEL4AABWCbX6DzgolJRYyyi1n1e+WFHEDSl2LaJbk8N6pq+n1X88AACPcTzP81ox8MZuTovJtFZ3spKkUqX64e+5oS55kuJjg5qbimp8ZKAVUwAA4AktCd/d+w+0sJxSsXKiRqM7juSGujU/E9Ptm8+ZngYAAE8w/lFnLXpbOi5XT/1Zz5OOyydaWN6SJOIHAGg5ow+3bOzmtLCcOlP0HnVcrmphOaXNTM7kdAAAeILR8C0m0ypWTpq6tlg50VIybXI6AAA8wVj4Dgolre5kG36n14jnSSvbWR0WSqamBADAE4yFL7GWCTyGIymxHnwcAACexlj4Uvv5x15ZaEaxUlVq78jQjAAAeJKx8OWLFUPjlI2MAwDAxzEWvohr5s2IiBs2Mg4AAB/HWPhi1yLqCQUbzg11KTbUb2hGAAA8yVj4ZieHA4/hSZqdCD4OAABPY2znlqt9PZoaHdTPt95v6pUGx5GmxwbZuBoADOCAgKczulfnxm5Or75+X8dl/y+x94a79ebXb+rF4QFT0wEA63BAwOmMb1LtZ6/Out5wl+ZnbrBXJwAEwAEBZ2N8k+r6v0T+5QPA+eGAgLNr2Xl8m5mclpJprWxn5aj2cnpdfbk9PTaouXiUjzcBIAC+ZvKnZeGrOyyUlFjPKLV3pHyxrIgbVmyoX7MTfMEKACZ8/Y13Az1Y+McvfFo/uP375id2QbU8fACA1jkolPTSX/1LoC0je0Jd+rc//0NrFiNGjyUCAJwvDgjwj/ABQAfjgAD/CB8AdDAOCPCP8AFAB+OAAP+Mv8cHADg/tQMC9gN93NnlVXSQ3tAvfzmoZ5991si8LvKWaTzVCQAdzMRTneEu6Q/23tI//UNCL7zwgm7duqXZ2dknIuh5nn71q181jGMnbJnGR50A0MHqBwQ4TnPXO4708o1P6+//9jXt7+/rO9/5jt577z199rOf1UsvvaTvf//7ymRqT3wmk0l95jOf0U9/+tOPHevu/Qd69fX7+vnW+ypVqk/EuPjBr9377/f16uv3dff+g+YmHRArPgDocK3YueXhw4d6++239ZOf/EQ/+9nPFIvFVC6X9e6778p1Xf3oRz/Sl770pQ9/vpP2aSZ8AHAJtDI8Dx8+1L179/TKK6+oXK49/RkOh7W0tKSvfe1rHbdlGh91AsAlcPvmc5qfuaHecPepH3s6Ti04Z11tXblyRX19fapWq/rkJz+pT3ziE6pUKvrWt74lz/O0mEyrWPEfPUkqVk60lEw3dW2zWPEBwCXSqgMCdnZ29MYbb+j69eu6fv26otGorl69qsP/e9hxW6YRPgC4hM7rgIAfrP6P/ubtnUDhc0Nd+rPPj+obn3ve2Lwa4T0+ALiEnunrOZeQdOKWaXzHBwBoWidumUb4AABN68Qt0wgfAKBptS3TgqXEDXUpNtRvaEanI3wAgKbNTg4HHsOTNDsRfJyzInwAgKaZ2DJtemzwXDeuJnwAgEDuxKNyQ91NXeuGujUXjxqeUWOEDwAQyPjIgOZnYuoN+0tKbcu02LluVybxHh8AwID61mcLyykVKydqtDWK49RWevMzsXPfoFpi5xYAgEGt2jLNJMIHADDuvLZMawbhAwBYhYdbAABWIXwAAKsQPgCAVQgfAMAqhA8AYBXCBwCwCju3wIiDQkmJtYxS+3nlixVF3JBi1yK6Ndn+d3YA4FG8x4dANnZzWkymtbqTlSSVPmaXhvjYoOamohofGWjPJAHgEYQPTbt7/0FH7MsHAI/io040pRa9LR2Xq6f+rOdJx+UTLSxvSRLxA9BWPNwC3zZ2c1pYTp0peo86Lle1sJzSZibXmokBwBkQPvi2mEyrWDlp6tpi5URLybThGQHA2RE++HJQKGl1J9vwO71GPE9a2c7qsFAyOzEAOCPCB18Sa5nAYziSEuvBxwGAZhA++JLazz/2ykIzipWqUntHhmYEAP4QPviSL1YMjVM2Mg4A+EX44EvENfMGTMQNGxkHAPwifPAldi2inlCwvzZuqEuxoX5DMwIAfwgffJmdHA48hidpdiL4OADQDMIHX6729WhqdFCO09z1jiNNjw2ycTWAtiF88O1OPCo31N3UtW6oW3PxqOEZAcDZET74Nj4yoPmZmHrD/v769Ia7ND8T04vDA62ZGACcAZtUoyn1jaY5nQFAp+FYIgSymclpKZnWynZWjmovp9fVz+ObHhvUXDzKSg/AhUD4YMRhoaTEekapvSPli2VF3LBiQ/2aneAEdgAXC+EDAFiFh1sAAFYhfAAAqxA+AIBVCB8AwCqEDwBgFcIHALAK4QMAWIXwAQCsQvgAAFYhfAAAqxA+AIBVCB8AwCqEDwBgFcIHALAK4QMAWIXwAQCsQvgAAFYhfAAAqxA+AIBVQu2eAAC0wkGhpMRaRqn9vPLFiiJuSLFrEd2aHNYzfT3tnh7ayPE8z2v3JADAlI3dnBaTaa3uZCVJpUr1w99zQ13yJMXHBjU3FdX4yEB7Jom2InwALo279x9oYTmlYuVEje5sjiO5oW7Nz8R0++ZzRv5sVpidg/ABuBRq0dvScbl6+g9/oDfcpfmZG4Hixwqz8xA+AB1vYzenV1+/r+Pyie9re8PdevPrN/Xi8IDva9u5wkTzeKoTQMdbTKZVrPiPniQVKydaSqZ9X/fRCrNx9CTJ86Tj8okWlrd09/6DpuYJcwgfgI52UChpdSd7anyexvOkle2sDgulM1+zsZvTwnLK18eqknRcrmphOaXNTM7nLGES4QPQ0RJrmcBjOJIS64+P89vf/lZvvvmmKpXKEz/fjhUmzCF8ADpaaj//2AMlzShWqkrtHT32a++8846+/OUv6/nnn9e9e/c+/PV2rDBhFi+wA+ho+eKTK7Jm/PPKv+qVN/9SkUhEn/rUp7S/v68rV67oF7/4hb74xS9qbGxMr732mt4rXg38Z9VXmN/43PPBJw7fCB+AjhZxzdzGXrxxXbc+/6zy+bx+85vfKJPJqFqtrSTL5bI2Nzf1ve99T0Ov/EVLVpg4P4QPQEeLXYuoJ7QfKEZuqEvTvzeqP3lkBdbT06O33npLfX19+uY3v6lvf/vbGh4e1lf/7h0T01a+WDYyDvzjOz4AHW12cjjwGJ6k2YnHx3n55Zf1wx/+UL/+9a/13e9+V8PDtd83tcKMuGEj48A/VnwAOtrVvh5NjQ7q51vvN/XAieNI02ODT2wrNjo6qtHR0Sd+3tQKMzbU3/T1CIYVH4COdycelRvqbupaN9StuXj0zD/fqhUmzg/hA9DxxkcGND8TU2/Y3y2ttldnzNd2ZfUVpuP4nOQHnrbCxPkhfAAuhds3n9P8zA31hrtPjZLj1PbobHaD6vNcYcI8NqkGcKlsZnJaSqa1sp2Vo9qrA3X10xKmxwY1F482tTF1XbtOg0BwhA/ApXRYKCmxnlFq70j5YlkRN6zYUL9mJ8ydj8fpDJ2J8AFAAOe1woQ5hA8ADDiPFSbMIHwAAKvwVCcAwCqEDwBgFcIHALAK4QMAWIXwAQCsQvgAAFYhfAAAqxA+AIBVCB8AwCqEDwBgFcIHALAK4QMAWIXwAQCsQvgAAFYhfAAAqxA+AIBVCB8AwCqEDwBgFcIHALAK4QMAWIXwAQCs8v+pQi8qb1m48wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "\n",
    "data = GraphData.random(5, 4, 3)\n",
    "\n",
    "# convert to a networkx object\n",
    "graph = data.to_networkx()\n",
    "\n",
    "# draw\n",
    "nx.draw(graph)\n",
    "\n",
    "# convert back to GraphData object\n",
    "from_nx = GraphData.from_networkx(graph)\n",
    "print(from_nx)\n",
    "print(data.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphBatches\n",
    "\n",
    "We can represent multiple graphs using a GraphBatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caldera.data import GraphBatch\n",
    "\n",
    "data_list = [GraphData.random(5, 4, 3), GraphData.random(5, 4, 3)]\n",
    "batch = GraphBatch.from_data_list(data_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Questions\n",
    "\n",
    "**How does caldera handle arbitrary sized graphs? I thought neural networks had to have defined input-output dimensions.**\n",
    "\n",
    "It does! However, Pytorch can handle arbitrary `batch` sizes. Lets check this out on a simple `torch.nn.Linear` module.\n",
    "\n",
    "**Does caldera do anything really special?**\n",
    "\n",
    "No not really. Just as PyTorch does not do anything really that special. They are both frameworks to generate and train neural networks. In both, the implementation and API are intimately linked to the construction of neural networks. Caldera has specialized classes and methods to handle graph-like data as torch.Tensor objects and specialized NN construction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6517,  0.8182],\n",
       "        [ 0.9411,  0.2952],\n",
       "        [-0.3964, -0.5660]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# we create some random data \n",
    "# we have a batch size of 3\n",
    "# and we have 10 features\n",
    "x = torch.randn(3, 10)\n",
    "lin = torch.nn.Linear(10, 2)\n",
    "lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [3 x 10], m2: [11 x 2] at /opt/conda/conda-bld/pytorch_1595629403081/work/aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-43f6e26a524c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mlin\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m11\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mlin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/caldera/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    720\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    721\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 722\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    724\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/caldera/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 91\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     92\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/caldera/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mlinear\u001B[0;34m(input, weight, bias)\u001B[0m\n\u001B[1;32m   1672\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m2\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mbias\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1673\u001B[0m         \u001B[0;31m# fused op is marginally faster\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1674\u001B[0;31m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maddmm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1675\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1676\u001B[0m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: size mismatch, m1: [3 x 10], m2: [11 x 2] at /opt/conda/conda-bld/pytorch_1595629403081/work/aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "# notice how this does not work\n",
    "# if there is a size mismatch\n",
    "x = torch.randn(3, 10)\n",
    "lin = torch.nn.Linear(11, 2)\n",
    "lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, knowing that Pytorch handles arbitrary dimensions (in the first dimension), we can batch together a graphs along their `node`, `edge` or `global` features.\n",
    "\n",
    "For example, lets say we have a graph with 2 nodes, 0 edges and a graph with 5 nodes, and 2 edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-91b27513e360>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-13-91b27513e360>\"\u001B[0;36m, line \u001B[0;32m26\u001B[0m\n\u001B[0;31m    assert edges.shape[0] = edges.shape[0]\u001B[0m\n\u001B[0m                          ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from caldera.data import GraphData\n",
    "\n",
    "\n",
    "nodes = torch.tensor([\n",
    "    [0, 0, 0],  # node 0\n",
    "    [1, 0, 0],  # node 1\n",
    "    [0, 1, 0],  # node 2\n",
    "    [0, 1, 0]   # node 3\n",
    "])\n",
    "edges = torch.tensor([\n",
    "    [0],  # edge 0\n",
    "    [0],  # edge 1\n",
    "    [1]   # edge 2\n",
    "])\n",
    "\n",
    "\n",
    "# edges 0-->1, 2-->3, 3-->0\n",
    "edges = torch.tensor([\n",
    "    \n",
    "    [0, 2, 3],\n",
    "    [1, 3, 0]\n",
    "#    ^  ^  ^\n",
    "#    0  1  2\n",
    "])\n",
    "\n",
    "assert edges.shape[0] = edges.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we batch graphs together? Lets just concatenate nodes, edges, and globals...\n",
    "\n",
    "We will keep track of which node belongs to which graph using `torch.LongTensor` (`node_idx`)\n",
    "and track which edge belongs to which graph using `edge_idx`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch modules\n",
    "\n",
    "Here we discuss how to inherit torch modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Graph Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks\n",
    "\n",
    "Here we discuss the basic building blocks for all of caldera's neural networks. The EdgeBlock, NodeBlock, and GlobalBlock. Within each block, we can have *arbitrary* modules. Really, these can be anything we want. \n",
    "\n",
    "#### Dense \n",
    "\n",
    "This is your basic multi-layer perceptron (a fancy term for a dense neural network). To begin, lets go over how pytorch modules and forward propogation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# initialize our toy network\n",
    "lin1 = nn.Linear(10, 5)\n",
    "act1 = nn.ReLU()\n",
    "lin2 = nn.Linear(5, 2)\n",
    "act2 = nn.ReLU()\n",
    "lin3 = nn.Linear(2, 5)\n",
    "act3 = nn.ReLU()\n",
    "lin4 = nn.Linear(5, 1)\n",
    "act4 = nn.Sigmoid()\n",
    "\n",
    "# some fake data\n",
    "x = torch.randn(20, 10) # note that the second dimension MUST be 10 since lin1 input dim is 10\n",
    "                        # 20 is the batch size and can be any number\n",
    "    \n",
    "# forward propogation\n",
    "print(x.shape)\n",
    "x = lin1(x)\n",
    "x = act1(x)\n",
    "x = lin2(x)\n",
    "x = act2(x)\n",
    "x = lin3(x)\n",
    "x = act3(x)\n",
    "x = lin4(x)\n",
    "x = act4(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do essentially the same thing using pytorch's standard Sequential model. The `Sequential` model takes a list of other modules and connects them together sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.3305],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "x = torch.randn(20, 10)\n",
    "layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **module composition** is very convinient as it keeps all of our code neat and tidy. Additionally, we have all of our parameters in one place. We can go one step further and create a whole new module to keep all of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dense(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, s1, s2):\n",
    "        super().__init__() # MUST be called to implement torch.nn.Module\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(s1, s2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(s2, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    # the built in method we need to implement\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "Dense(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2983],\n",
       "        [0.0263],\n",
       "        [0.2283],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.3209],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.2197],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.2245],\n",
       "        [0.0006],\n",
       "        [0.1633],\n",
       "        [0.0207],\n",
       "        [0.0477],\n",
       "        [0.0945],\n",
       "        [0.3365],\n",
       "        [0.1346],\n",
       "        [0.3091]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(10, 5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flex block and flexible dimensions\n",
    "\n",
    "One of the most fustrating aspects of complex neural networks is getting the **size** correct. This is particularly troublesome with some of the neural networks Caldera creates.\n",
    "\n",
    "To overcome this, Caldera has a small module called a `FlexBlock` that automatically infers dimensions.\n",
    "\n",
    "We can convert almost any pytorch module into a flexible module by calling `Flex(<ModuleType>)`. Lets start by making a flexible Linear module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────── standard version ─────────────────────────────────────╮\n",
       "│Linear(in_features=5, out_features=10, bias=True)                                          │\n",
       "╰───────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fe4511244d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000\">╭────────────────────────────── flexible version (unresolved) ──────────────────────────────╮</span>\n",
       "<span style=\"color: #800000\">│FlexBlock(                                                                                 │</span>\n",
       "<span style=\"color: #800000\">│        (unresolved_module): Linear(FlexDim(pos=0, dim=1), 10,                             │</span>\n",
       "<span style=\"color: #800000\">│)                                                                                          │</span>\n",
       "<span style=\"color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fe45113d8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from caldera.blocks import Flex\n",
    "from rich.panel import Panel\n",
    "from rich import print\n",
    "from rich.console import Console\n",
    "FlexLinear = Flex(nn.Linear)\n",
    "flex_lin1 =  FlexLinear(..., 10)\n",
    "lin1 = nn.Linear(5, 10)\n",
    "\n",
    "print(Panel(str(lin1), title='standard version'))\n",
    "\n",
    "print(Panel(str(flex_lin1), title='flexible version (unresolved)', style='red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the word `unresolved_module` in the FlexBlock. This means we *do not yet know the dimenions* of this module. To *resolve* the module, all we have to do is pass in some data. This will resolve the dimensions at the position indicated by the `...` argument.\n",
    "\n",
    "The `...` is just syntatic sugar to define a `FlexDim` instance. FlexDim will resolve the indicated argument position `pos` using the input data dimension `dim`. By default, when `...` is provided, it will default to its own arg position and dim=1. In most cases, just using `...` at the position you want to be flexible will suffice. Use `FlexDim` if there are multiple flexible dimensions and multiple flexible positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000\">╭─────────────────────────────── flexible version (resolved) ───────────────────────────────╮</span>\n",
       "<span style=\"color: #008000\">│FlexBlock(                                                                                 │</span>\n",
       "<span style=\"color: #008000\">│  (resolved_module): Linear(in_features=5, out_features=10, bias=True)                     │</span>\n",
       "<span style=\"color: #008000\">│)                                                                                          │</span>\n",
       "<span style=\"color: #008000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fe4504e6290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flex_lin1(torch.randn(10, 5))\n",
    "print(Panel(str(flex_lin1), title='flexible version (resolved)', style='green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we now have a Linear layer with expected shape 5. This means the module is resolved and can be used as normal. The flex block if powerful if used in module compositions. For example, in sequential compositions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ellipsis"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Sequential(\n",
    "    Flex(nn.Linear)(..., 10),\n",
    "    nn.ReLU(),\n",
    "    Flex(nn.Linear)(..., 4),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message passing (how we pass information throughout a graph neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to we pass data in the graph? We will use graph neighbors to pass information to the current node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message passing and representation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-24-38a06e69d475>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReLU\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
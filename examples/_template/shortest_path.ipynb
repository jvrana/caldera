{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortest-Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix path and import caldera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, isfile\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def find_pkg(name: str, depth: int):\n",
    "    if depth <= 0:\n",
    "        ret = None\n",
    "    else:\n",
    "        d = ['..'] * depth\n",
    "        path_parts = d + [name, '__init__.py']\n",
    "        \n",
    "        if isfile(join(*path_parts)):\n",
    "            ret = d\n",
    "        else:\n",
    "            ret = find_pkg(name, depth-1)\n",
    "    return ret\n",
    "\n",
    "def find_and_ins_syspath(name: str, depth: int):\n",
    "    path_parts = find_pkg(name, depth)\n",
    "    if path_parts is None:\n",
    "        raise RuntimeError(\"Could not find {}. Try increasing depth.\".format(name))\n",
    "    path = join(*path_parts)\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)\n",
    "\n",
    "try:\n",
    "    import caldera\n",
    "except ImportError:\n",
    "    find_and_ins_syspath('caldera', 3)\n",
    "\n",
    "import caldera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "Generate and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pass # generate data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Network\n",
    "\n",
    "We are going to build a graph network to handle the data we just created.\n",
    "\n",
    "We are going to use a flexible `encoder -> core[x] -> decoder` architecture for this problem. The architecture consists of 4 main networks, the **encoder**, **core**, **decoder**, and **output_transform** networks. The **encoder** encodes graph data inputs into arbitrary shapes. The **core** is the central graph message processing network. The **decoder** decodes encoded data. Finally, the **output_transform** transformed decoded data for final output. \n",
    "\n",
    "Setting up the network looks like the following:\n",
    "\n",
    "``` python\n",
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(...):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.config = {...}\n",
    "\n",
    "        self.encoder = ...\n",
    "        self.core = ...\n",
    "        self.decoder = ...\n",
    "        self.out_transform = ...  \n",
    "\n",
    "    def forward(self, data, steps, save_all: bool = False):\n",
    "        \"\"\"The encoder -> core -> decode loop\"\"\"\n",
    "        encoded = self.encoder(data) # encode data\n",
    "        \n",
    "        outputs = []\n",
    "        for _ in range(steps):\n",
    "            latent = self.core(encoded)\n",
    "            encoded = self.decoder(latent)\n",
    "            outputs.append(self.out_transform(latent)\n",
    "        return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flex Modules and Flex Dimensions\n",
    "\n",
    "Setting up this network with the correct dimensions can become tricky, so we introduce a new module, the `Flex` module, which can resolve unknown dimensions on runtime. To make a module a `Flex` module, we just call `Flex` with any `torch.nn.Module`, as in `Flex(torch.nn.Linear)` or `Flex(MyAwesomeModule)`. To initialize the module with unknown dimensions, you use the flexible dimension object `Flex.d` in places where the dimension is to be resolve on runtime, as in `Flex(torch.nn.Linear)(Flex.d(), 10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'caldera'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-976b198c6e3d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mcaldera\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mblocks\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mFlex\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mFlexLinear\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFlex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLinear\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'caldera'"
     ]
    }
   ],
   "source": [
    "from caldera.blocks import Flex\n",
    "import torch\n",
    "\n",
    "FlexLinear = Flex(torch.nn.Linear)\n",
    "\n",
    "linear0 = torch.nn.Linear(3, 10)\n",
    "flex_linear0 = FlexLinear(Flex.d(), 10)\n",
    "print(linear0)\n",
    "print(flex_linear0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice that the FlexBlock indicates it is current unresolved. To resovle it, we need to provide it with a data example. You'll see the module is now resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-3c74c6835025>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mexample\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mflex_linear0\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexample\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflex_linear0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "example = torch.zeros((1, 10))\n",
    "\n",
    "flex_linear0(example)\n",
    "\n",
    "print(flex_linear0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aggregators\n",
    "\n",
    "Aggregators are layers that indicate how data is processed and aggregated between neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Network\n",
    "\n",
    "Build the final network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from caldera.blocks import NodeBlock, EdgeBlock, GlobalBlock\n",
    "from caldera.blocks import AggregatingNodeBlock, AggregatingEdgeBlock, AggregatingGlobalBlock\n",
    "from caldera.blocks import MultiAggregator\n",
    "from caldera.blocks import Flex\n",
    "from caldera.models import GraphCore, GraphEncoder\n",
    "from caldera.defaults import CalderaDefaults as defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from caldera.defaults import CalderaDefaults as defaults\n",
    "from caldera.blocks import Flex, NodeBlock, EdgeBlock, GlobalBlock, MLP, AggregatingEdgeBlock, AggregatingNodeBlock, \\\n",
    "    MultiAggregator, AggregatingGlobalBlock\n",
    "from caldera.models import GraphEncoder, GraphCore\n",
    "from caldera.data import GraphBatch\n",
    "\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            latent_sizes=(32, 32, 32),\n",
    "            out_sizes=(1, 1, 1),\n",
    "            latent_depths=(1, 1, 1),\n",
    "            dropout: float = None,\n",
    "            pass_global_to_edge: bool = True,\n",
    "            pass_global_to_node: bool = True,\n",
    "            activation=defaults.activation,\n",
    "            out_activation=defaults.activation,\n",
    "            edge_to_node_aggregators=tuple([\"add\", \"max\", \"mean\", \"min\"]),\n",
    "            edge_to_global_aggregators=tuple([\"add\", \"max\", \"mean\", \"min\"]),\n",
    "            node_to_global_aggregators=tuple([\"add\", \"max\", \"mean\", \"min\"]),\n",
    "            aggregator_activation=defaults.activation,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = {\n",
    "            \"sizes\": {\n",
    "                'latent': {\n",
    "                    \"edge\": latent_sizes[0],\n",
    "                    \"node\": latent_sizes[1],\n",
    "                    \"global\": latent_sizes[2],\n",
    "                    \"edge_depth\": latent_depths[0],\n",
    "                    \"node_depth\": latent_depths[1],\n",
    "                    \"global_depth\": latent_depths[2],\n",
    "                },\n",
    "                'out': {\n",
    "                    'edge': out_sizes[0],\n",
    "                    'node': out_sizes[1],\n",
    "                    'global': out_sizes[2],\n",
    "                    'activation': out_activation,\n",
    "                }\n",
    "            },\n",
    "            'activation': activation,\n",
    "            \"dropout\": dropout,\n",
    "            \"node_block_aggregator\": edge_to_node_aggregators,\n",
    "            \"global_block_to_node_aggregator\": node_to_global_aggregators,\n",
    "            \"global_block_to_edge_aggregator\": edge_to_global_aggregators,\n",
    "            \"aggregator_activation\": aggregator_activation,\n",
    "            \"pass_global_to_edge\": pass_global_to_edge,\n",
    "            \"pass_global_to_node\": pass_global_to_node,\n",
    "        }\n",
    "\n",
    "        ###########################\n",
    "        # encoder\n",
    "        ###########################\n",
    "\n",
    "        self.encoder = self._init_encoder()\n",
    "        self.core = self._init_core()\n",
    "        self.decoder = self._init_encoder()\n",
    "        self.output_transform = self._init_out_transform()\n",
    "\n",
    "        self.output_transform = GraphEncoder(\n",
    "            EdgeBlock(\n",
    "                torch.nn.Sequential(\n",
    "                    Flex(torch.nn.Linear)(Flex.d(), 1), torch.nn.Sigmoid()\n",
    "                )\n",
    "            ),\n",
    "            NodeBlock(\n",
    "                torch.nn.Sequential(\n",
    "                    Flex(torch.nn.Linear)(Flex.d(), 1), torch.nn.Sigmoid()\n",
    "                )\n",
    "            ),\n",
    "            GlobalBlock(Flex(torch.nn.Linear)(Flex.d(), 1)),\n",
    "        )\n",
    "\n",
    "    def _init_encoder(self):\n",
    "        return GraphEncoder(\n",
    "            EdgeBlock(Flex(MLP)(Flex.d(), self.config['sizes']['latent']['edge'], dropout=self.config['dropout'])),\n",
    "            NodeBlock(Flex(MLP)(Flex.d(), self.config['sizes']['latent']['node'], dropout=self.config['dropout'])),\n",
    "            GlobalBlock(Flex(MLP)(Flex.d(), self.config['sizes']['latent']['global'], dropout=self.config['dropout'])),\n",
    "        )\n",
    "\n",
    "    def _init_core(self):\n",
    "        edge_layers = [self.config['sizes']['latent']['edge']] * self.config['sizes']['latent']['edge_depth']\n",
    "        node_layers = [self.config['sizes']['latent']['node']] * self.config['sizes']['latent']['node_depth']\n",
    "        global_layers = [self.config['sizes']['latent']['global']] * self.config['sizes']['latent']['global_depth']\n",
    "\n",
    "        return GraphCore(\n",
    "            AggregatingEdgeBlock(\n",
    "                torch.nn.Sequential(\n",
    "                    Flex(MLP)(Flex.d(), *edge_layers, dropout=self.config['dropout'], layer_norm=True),\n",
    "                )\n",
    "            ),\n",
    "            AggregatingNodeBlock(\n",
    "                torch.nn.Sequential(\n",
    "                    Flex(MLP)(Flex.d(), *node_layers, dropout=self.config['dropout'], layer_norm=True),\n",
    "                ),\n",
    "                Flex(MultiAggregator)(\n",
    "                    Flex.d(),\n",
    "                    self.config[\"node_block_aggregator\"],\n",
    "                    activation=self.config[\"aggregator_activation\"],\n",
    "                ),\n",
    "            ),\n",
    "            AggregatingGlobalBlock(\n",
    "                torch.nn.Sequential(\n",
    "                    Flex(MLP)(\n",
    "                        Flex.d(), *global_layers, dropout=self.config['dropout'], layer_norm=True\n",
    "                    ),\n",
    "                ),\n",
    "                edge_aggregator=Flex(MultiAggregator)(\n",
    "                    Flex.d(),\n",
    "                    self.config[\"global_block_to_edge_aggregator\"],\n",
    "                    activation=self.config[\"aggregator_activation\"],\n",
    "                ),\n",
    "                node_aggregator=Flex(MultiAggregator)(\n",
    "                    Flex.d(),\n",
    "                    self.config[\"global_block_to_node_aggregator\"],\n",
    "                    activation=self.config[\"aggregator_activation\"],\n",
    "                ),\n",
    "            ),\n",
    "            pass_global_to_edge=self.config[\"pass_global_to_edge\"],\n",
    "            pass_global_to_node=self.config[\"pass_global_to_node\"],\n",
    "        )\n",
    "\n",
    "    def _init_out_transform(self):\n",
    "        return GraphEncoder(\n",
    "            EdgeBlock(\n",
    "                torch.nn.Sequential(\n",
    "                    Flex(torch.nn.Linear)(Flex.d(), self.config['sizes']['out']['edge']),\n",
    "                    self.config['sizes']['out']['activation']()\n",
    "                )\n",
    "            ),\n",
    "            NodeBlock(\n",
    "                torch.nn.Sequential(\n",
    "                    Flex(torch.nn.Linear)(Flex.d(), self.config['sizes']['out']['node']),\n",
    "                    self.config['sizes']['out']['activation']()\n",
    "                )\n",
    "            ),\n",
    "            GlobalBlock(\n",
    "                torch.nn.Sequential(\n",
    "                    Flex(torch.nn.Linear)(Flex.d(), self.config['sizes']['out']['global']),\n",
    "                    self.config['sizes']['out']['activation']()\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _forward_encode(self, data):\n",
    "        e, x, g = self.encoder(data)\n",
    "        return GraphBatch(x, e, g, data.edges, data.node_idx, data.edge_idx)\n",
    "\n",
    "    def _forward_decode(self, data):\n",
    "        e, x, g = self.decoder(data)\n",
    "        return GraphBatch(x, e, g, data.edges, data.node_idx, data.edge_idx)\n",
    "\n",
    "    def _forward_core(self, latent0, data):\n",
    "        e = torch.cat([latent0.e, data.e], dim=1)\n",
    "        x = torch.cat([latent0.x, data.x], dim=1)\n",
    "        g = torch.cat([latent0.g, data.g], dim=1)\n",
    "        data = GraphBatch(x, e, g, data.edges, data.node_idx, data.edge_idx)\n",
    "        e, x, g = self.core(data)\n",
    "        return GraphBatch(x, e, g, data.edges, data.node_idx, data.edge_idx)\n",
    "\n",
    "    def _forward_out(self, data):\n",
    "        e, x, g = self.output_transform(data)\n",
    "        return GraphBatch(x, e, g, data.edges, data.node_idx, data.edge_idx)\n",
    "\n",
    "    def forward(self, data, steps, save_all: bool = False):\n",
    "        data = self._forward_encode(data)\n",
    "        latent0 = data\n",
    "\n",
    "        outputs = []\n",
    "        for _ in range(steps):\n",
    "            data = self._forward_core(latent0, data)\n",
    "            data = self._forward_decode(data)\n",
    "            out_data = self._forward_out(data)\n",
    "            if save_all:\n",
    "                outputs.append(out_data)\n",
    "            else:\n",
    "                outputs = [out_data]\n",
    "        return outputs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Provide example to resolve Flex modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create loaders for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_datalist = [GraphData.from_networkx(g, feature_key='_features') for g in nx_graphs]\n",
    "target_datalist = [GraphData.from_networkx(g, feature_key='_target') for g in nx_graphs]\n",
    "eval_input_datalist = [GraphData.from_networkx(g, feature_key='_features') for g in eval_graphs]\n",
    "eval_target_datalist = [GraphData.from_networkx(g, feature_key='_target') for g in eval_graphs]\n",
    "loader = GraphDataLoader(input_datalist, target_datalist, batch_size=512)\n",
    "eval_loader = GraphDataLoader(eval_input_datalist, eval_target_datalist, batch_size=len(eval_input_datalist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/jvrana/caldera-shortest-path\" target=\"_blank\">https://app.wandb.ai/jvrana/caldera-shortest-path</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/jvrana/caldera-shortest-path/runs/248qqija\" target=\"_blank\">https://app.wandb.ai/jvrana/caldera-shortest-path/runs/248qqija</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6db09c49be441a19e69c1a7ca29c330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    project=...,\n",
    "    tags=...\n",
    "    group=...\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# get device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda available\")\n",
    "    cuda_device = torch.cuda.current_device()\n",
    "    device = 'cuda:' + str(cuda_device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# initialize network    \n",
    "network = Network()\n",
    "\n",
    "# resolve\n",
    "for input_batch, _ in loader:\n",
    "    x = input_batch.x\n",
    "    network(input_batch, 10)\n",
    "    break\n",
    "    \n",
    "# send to device\n",
    "network.to(device, non_blocking=True)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(network.parameters())\n",
    "\n",
    "# TODO: loss should use all output to accomplish goal in min steps\n",
    "# training loop\n",
    "num_epochs = 1000\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = torch.tensor(0., device=device)\n",
    "    for input_batch, target_batch in loader:\n",
    "        network.train()\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        \n",
    "        output = network(input_batch, 10)[0]\n",
    "        x, y = output.x, target_batch.x\n",
    "        loss = loss_fn(x.flatten(), y[:, 0].flatten())\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss += loss\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    wandb.log({'train_loss': running_loss.detach().cpu().item()}, step=epoch)\n",
    "    if epoch % 10:\n",
    "        network.eval()\n",
    "        running_eval_loss = 0.\n",
    "        for eval_input_batch, eval_target_batch in eval_loader:\n",
    "            eval_input_batch = eval_input_batch.to(device)\n",
    "            eval_target_batch = eval_target_batch.to(device)\n",
    "                \n",
    "            eval_out = network(eval_input_batch, 10)[0]\n",
    "            x, y = eval_out.x, eval_target_batch.x\n",
    "            eval_loss = loss_fn(x.flatten(), y[:, 0].flatten())\n",
    "            running_eval_loss += eval_loss.detach().cpu().item()\n",
    "        wandb.log({\"eval_loss\": running_eval_loss}, step=epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>caldera.utils &#8212; caldera 0.1.0 documentation</title>
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/copybutton.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="caldera.utils.dict_join" href="generated/caldera.utils.dict_join.html" />
    <link rel="prev" title="caldera.transforms" href="caldera.transforms.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../_static/bootstrap-sphinx.js "></script>

  </head><body>



  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html">
          Caldera</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../getting_started.html">Getting Started</a></li>
                <li><a href="../examples/examples.html">Examples</a></li>
                <li><a href="../api.html">API</a></li>
                <li><a href="http://www.github.com/jvrana/caldera">Github</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Caldera <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../getting_started.html#installation">Installation</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="caldera.data.html">caldera.data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="caldera.data.html#data-caldera-data">Data (<code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.data</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="caldera.blocks.html">caldera.blocks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="caldera.blocks.html#blocks-caldera-blocks">Blocks (<code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.blocks</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="caldera.models.html">caldera.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="caldera.models.html#data-caldera-models">Data (<code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.models</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="caldera.transforms.html">caldera.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="caldera.transforms.html#transforms-caldera-transform">Transforms (<code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.transform</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">caldera.utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#utils-caldera-utils">Utils (<code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.utils</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="caldera.exceptions.html">caldera.exceptions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="caldera.exceptions.html#exceptions-caldera-exceptions">Exceptions (<code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.exceptions</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="caldera.defaults.html">caldera.defaults</a><ul>
<li class="toctree-l3"><a class="reference internal" href="caldera.defaults.html#defaults-caldera-defaults">Defaults (<code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.defaults</span></code>)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/examples.html">Examples</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">caldera.utils</a><ul>
<li><a class="reference internal" href="#utils-caldera-utils">Utils (<code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.utils</span></code>)</a><ul>
<li><a class="reference internal" href="#indexing">Indexing</a></li>
<li><a class="reference internal" href="#tensor">Tensor</a></li>
<li><a class="reference internal" href="#functional">Functional</a></li>
<li><a class="reference internal" href="#networkx-utilities">Networkx Utilities</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <div class="section" id="module-caldera.utils">
<span id="caldera-utils"></span><h1>caldera.utils<a class="headerlink" href="#module-caldera.utils" title="Permalink to this headline">¶</a></h1>
<div class="section" id="utils-caldera-utils">
<h2>Utils (<a class="reference internal" href="#module-caldera.utils" title="caldera.utils"><code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.utils</span></code></a>)<a class="headerlink" href="#utils-caldera-utils" title="Permalink to this headline">¶</a></h2>
<p>Caldera utility functions.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/caldera.utils.dict_join.html#caldera.utils.dict_join" title="caldera.utils.dict_join"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dict_join</span></code></a>(a, b[, out, join_fn, default_a, …])</p></td>
<td><p>Join two dictionaries.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="indexing">
<h3>Indexing<a class="headerlink" href="#indexing" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/caldera.utils.reindex_tensor.html#caldera.utils.reindex_tensor" title="caldera.utils.reindex_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reindex_tensor</span></code></a>(a)</p></td>
<td><p>Reindex a tensor to lowest index.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/caldera.utils.unravel_index.html#caldera.utils.unravel_index" title="caldera.utils.unravel_index"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unravel_index</span></code></a>(index, shape)</p></td>
<td><p><dl class="field-list simple">
<dt class="field-odd">rtype</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LongTensor</span></code>, …]</p>
</dd>
</dl>
</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="tensor">
<h3>Tensor<a class="headerlink" href="#tensor" title="Permalink to this headline">¶</a></h3>
<p>Utilities for <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/caldera.utils.scatter_coo.html#caldera.utils.scatter_coo" title="caldera.utils.scatter_coo"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_coo</span></code></a>(indices, source[, size, expand, …])</p></td>
<td><p>Scatter the provided source tensor to the provided indices.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/caldera.utils.scatter_indices.html#caldera.utils.scatter_indices" title="caldera.utils.scatter_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scatter_indices</span></code></a>(indices, shape)</p></td>
<td><p>Unroll the coo indices using the provided shape.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/caldera.utils.torch_coo_to_scipy_coo.html#caldera.utils.torch_coo_to_scipy_coo" title="caldera.utils.torch_coo_to_scipy_coo"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch_coo_to_scipy_coo</span></code></a>(m)</p></td>
<td><p>Convert torch <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.sparse.FloatTensor</span></code></a> tensor to.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/caldera.utils.deterministic_seed.html#caldera.utils.deterministic_seed" title="caldera.utils.deterministic_seed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">deterministic_seed</span></code></a>(seed[, cudnn_deterministic])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/caldera.utils.long_isin.html#caldera.utils.long_isin" title="caldera.utils.long_isin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">long_isin</span></code></a>(ar1, ar2[, assume_unique, invert])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/caldera.utils.same_storage.html#caldera.utils.same_storage" title="caldera.utils.same_storage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">same_storage</span></code></a>(x, y[, …])</p></td>
<td><p>Checks if two tensors share storage.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/caldera.utils.stable_arg_sort_long.html#caldera.utils.stable_arg_sort_long" title="caldera.utils.stable_arg_sort_long"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stable_arg_sort_long</span></code></a>(arr)</p></td>
<td><p>Stable sort of long tensors.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/caldera.utils.tensor_is_empty.html#caldera.utils.tensor_is_empty" title="caldera.utils.tensor_is_empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensor_is_empty</span></code></a>(x)</p></td>
<td><p>Return whether the tensor is empty.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/caldera.utils.torch_scatter_group.html#caldera.utils.torch_scatter_group" title="caldera.utils.torch_scatter_group"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch_scatter_group</span></code></a>(x, idx)</p></td>
<td><p>Group a tensor by indices.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="functional">
<h3>Functional<a class="headerlink" href="#functional" title="Permalink to this headline">¶</a></h3>
<p>Functional programming module.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/caldera.utils.functional.html#module-caldera.utils.functional" title="caldera.utils.functional"><code class="xref py py-obj docutils literal notranslate"><span class="pre">functional</span></code></a></p></td>
<td><p>functional.py.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="networkx-utilities">
<h3>Networkx Utilities<a class="headerlink" href="#networkx-utilities" title="Permalink to this headline">¶</a></h3>
<p>Extra <code class="xref py py-mod docutils literal notranslate"><span class="pre">networkx</span></code> utilities</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/caldera.utils.nx.html#module-caldera.utils.nx" title="caldera.utils.nx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nx</span></code></a></p></td>
<td><p>Networkx Utilities (<a class="reference internal" href="generated/caldera.utils.nx.html#module-caldera.utils.nx" title="caldera.utils.nx"><code class="xref py py-mod docutils literal notranslate"><span class="pre">caldera.utils.nx</span></code></a>)</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt id="caldera.utils.reindex_tensor">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">reindex_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">tensors</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.reindex_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Reindex a tensor to lowest index. Handles multiple tensors and tensors
with many dimensions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">reindex</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="c1"># &gt;&gt; tensor([0, 0, 0, 1, 2, 3, 2, 2, 2])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># multiple tensors with multiple dimensions</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">70</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">expected1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">expected2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">reindex_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">c</span> <span class="o">==</span> <span class="n">expected1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">d</span> <span class="o">==</span> <span class="n">expected2</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>a</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>) – tensor to reindex</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, …]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>new reindexed tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="caldera.utils.scatter_coo">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">scatter_coo</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">source</span></em>, <em class="sig-param"><span class="n">size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">expand</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.scatter_coo" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatter the provided source tensor to the provided indices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">LongTensor</span></code>) – </p></li>
<li><p><strong>source</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">FloatTensor</span></code>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">FloatTensor</span></code></a></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="caldera.utils.scatter_indices">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">scatter_indices</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.scatter_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Unroll the coo indices using the provided shape.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scatter_indices</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">shape</span><span class="p">))</span>

<span class="c1"># tensor([[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2,</span>
<span class="c1">#  0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],</span>
<span class="c1"># [2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4,</span>
<span class="c1">#  2, 3, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4],</span>
<span class="c1"># [4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4,</span>
<span class="c1">#  4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 4],</span>
<span class="c1"># [0, 0, 1, 1, 2, 2, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 2, 2, 0, 1, 0, 1, 0, 1,</span>
<span class="c1">#  0, 0, 1, 1, 2, 2, 0, 1, 0, 1, 0, 1]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">LongTensor</span></code>) – </p></li>
<li><p><strong>shape</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Size</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, …]]) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="caldera.utils.torch_coo_to_scipy_coo">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">torch_coo_to_scipy_coo</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">m</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.torch_coo_to_scipy_coo" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert torch <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html#torch.sparse.FloatTensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.sparse.FloatTensor</span></code></a> tensor to.</p>
<p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix" title="(in SciPy v1.5.2)"><code class="xref py py-class docutils literal notranslate"><span class="pre">scipy.sparse.coo_matrix</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">coo_matrix</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="caldera.utils.same_storage">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">same_storage</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em>, <em class="sig-param"><span class="n">empty_does_not_share_storage</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.same_storage" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if two tensors share storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>) – first tensor</p></li>
<li><p><strong>y</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>) – second tensor</p></li>
<li><p><strong>empty_does_not_share_storage</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True (default), will return False if
either tensor is empty (despite that they technically data_ptr are the same).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>if the tensor shares the same storage</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="caldera.utils.stable_arg_sort_long">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">stable_arg_sort_long</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">arr</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.stable_arg_sort_long" title="Permalink to this definition">¶</a></dt>
<dd><p>Stable sort of long tensors.</p>
<p>Note that Pytorch 1.5.0 does not have a stable sort implementation.
Here we simply add a delta value between 0 and 1 (exclusive) and
assuming we are using integers, call torch.argsort to get a stable
sort.</p>
</dd></dl>

<dl class="py function">
<dt id="caldera.utils.tensor_is_empty">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">tensor_is_empty</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.tensor_is_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Return whether the tensor is empty.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="caldera.utils.torch_scatter_group">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">torch_scatter_group</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">idx</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.torch_scatter_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Group a tensor by indices. This is equivalent to successive applications
of <cite>x[torch.where(x == index)]</cite> for all provided sorted indices.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>

<span class="n">uniq_sorted_idx</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">scatter_group</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>

<span class="c1"># node the idx is sorted</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])))</span>

<span class="c1"># where idx == 0</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">])))</span>

<span class="c1"># where idx == 1</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])))</span>

<span class="c1"># where idx == 2</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">])))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>) – tensor to group</p></li>
<li><p><strong>idx</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>) – indices</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch vmaster (1.6.0a0+4392e52 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>tuple of unique, sorted indices and a list of tensors corresponding to the groups</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="caldera.utils.dict_join">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">dict_join</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em>, <em class="sig-param"><span class="n">out</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">join_fn</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">default_a</span><span class="o">=</span><span class="default_value">Ellipsis</span></em>, <em class="sig-param"><span class="n">default_b</span><span class="o">=</span><span class="default_value">Ellipsis</span></em>, <em class="sig-param"><span class="n">keys</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mode</span><span class="o">=</span><span class="default_value">'union'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.dict_join" title="Permalink to this definition">¶</a></dt>
<dd><p>Join two dictionaries. This function merges two dictionaries is various
ways. For example, a dictionary of <cite>Dict[str, List]</cite> can be merged such
that, if the two dictionaries share the same key, the lists are
concatenated. The join function can be applied to the union of all keys
(default) by (<cite>mode=”union”</cite>), the intersection of the dictionary
(<cite>mode=”intersection”), only the keys in the left dictionary
(`mode=”left”</cite>), or keys only in the right dictionary (<cite>mode=”right”</cite>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">operator</span>
<span class="n">d1</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">d2</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">]</span>
<span class="n">d3</span> <span class="o">=</span> <span class="n">dict_join</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">join_fn</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d3</span><span class="p">)</span>
<span class="c1"># {&#39;a&#39;: [1,2,3,1,2], &#39;b&#39;: [1,2], &#39;c&#39;: [10,20]}</span>
</pre></div>
</div>
<p>This can be done such that the first dictionary is updated instead of returning a new dictionary:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d1</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">d2</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">]</span>
<span class="n">dict_join</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">join_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d2</span><span class="p">)</span>
<span class="c1"># {&#39;a&#39;: [1,2,3,1,2], &#39;b&#39;: [1,2], &#39;c&#39;: [10,20]}</span>

<span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">dict_join</span><span class="p">,</span> <span class="n">join_fv</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">add</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[~K, ~T]) – First dictionary</p></li>
<li><p><strong>b</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[~K, ~S]) – Second dictionary</p></li>
<li><p><strong>out</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – The target dictionary. If None, creates a new dictionary.</p></li>
<li><p><strong>join_fn</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[~T, ~S], ~V]]) – Join function when both dictionaries share the same key. If not provided, will use the value
provided by the second dictionary.</p></li>
<li><p><strong>default_a</strong> (<em>~T</em>) – Default value to use in the case a key is missing from the first dictionary. If defined as
<cite>Ellipsis</cite> (or <cite>…</cite>), defaults will be ignored.</p></li>
<li><p><strong>default_b</strong> (<em>~S</em>) – Default value to use in the case a key is missing from the second dictionary. If defined as
<cite>Ellipsis</cite> (or <cite>…</cite>), defaults will be ignored.</p></li>
<li><p><strong>keys</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Iterable</span></code>[~K]]) – If provided, explicitly join only on specified keys.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – If keys are None, mode specifies which keys to join. “union” (default), means join
takes the union of keys from both dictionaries. “intersection” means take intersection of keys
for both dictionaries. “left” means use only keys in the first dictionary. “right” means use only keys
from the second dictionary.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[~K, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[~T, ~S, ~V]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="caldera.utils.pairwise">
<code class="sig-prename descclassname">caldera.utils.</code><code class="sig-name descname">pairwise</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">iterable</span></em><span class="sig-paren">)</span><a class="headerlink" href="#caldera.utils.pairwise" title="Permalink to this definition">¶</a></dt>
<dd><p>s -&gt; (s0,s1), (s1,s2), (s2, s3), …</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterable</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[~T, ~T]]</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
        <br/>
        
<div id="sourcelink">
  <a href="../_sources/generated/caldera.utils.rst.txt"
     rel="nofollow">Source</a>
</div>
      
    </p>
    <p>
        &copy; Copyright 2020, Justin Vrana &lt;justin.vrana@gmail.com&gt;.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>